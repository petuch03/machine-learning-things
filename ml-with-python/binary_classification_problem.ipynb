{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "\n",
    "<a href=\"https://colab.research.google.com/github/petuch03/data-science-things/blob/master/ml_with_python/binary_classification_problem.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "<a href=\"https://www.kaggle.com/competitions/nup-ml-1-2023-competition/leaderboard\" target=\"_parent\">Kaggle Competition</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model without the least important feature, acc = 0.942"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 180 candidates, totalling 900 fits\n",
      "Best Parameters: {'learning_rate': 0.05, 'max_depth': 15, 'n_estimators': 200, 'subsample': 0.9}\n",
      "      Feature  Importance\n",
      "9   feature12    0.190522\n",
      "1    feature1    0.165431\n",
      "0    feature8    0.110811\n",
      "2    feature3    0.098326\n",
      "5    feature4    0.091025\n",
      "4    feature7    0.083082\n",
      "6   feature10    0.075563\n",
      "3    feature5    0.068044\n",
      "7   feature11    0.048532\n",
      "11   feature9    0.025337\n",
      "10   feature6    0.022957\n",
      "8    feature2    0.020371\n",
      "Accuracy: 0.92\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import xgboost as xgb\n",
    "\n",
    "data = pd.read_csv('binary_classification_datasets/train.csv')\n",
    "data.fillna(data.mean(), inplace=True)\n",
    "\n",
    "# Feature selection based on importance (previous launches):\n",
    "# 7    feature8    0.187670\n",
    "# 0    feature1    0.113971\n",
    "# 2    feature3    0.101499\n",
    "# 4    feature5    0.098769\n",
    "# 6    feature7    0.092911\n",
    "# 3    feature4    0.069674\n",
    "# 9   feature10    0.064359\n",
    "# 10  feature11    0.060138\n",
    "# 1    feature2    0.054287\n",
    "# 11  feature12    0.050472\n",
    "# 5    feature6    0.046922\n",
    "# 8    feature9    0.046358\n",
    "# 12  feature13    0.012970\n",
    "important_features = ['feature8', 'feature1', 'feature3', 'feature5', 'feature7',\n",
    "                      'feature4', 'feature10', 'feature11', 'feature2', 'feature12', \n",
    "                      'feature6', 'feature9']\n",
    "X = data[important_features]\n",
    "y = data['target']\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=30)\n",
    "\n",
    "# Feature scaling\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# XGBoost model\n",
    "model = xgb.XGBClassifier()\n",
    "\n",
    "# Hyperparameters grid\n",
    "param_grid = {\n",
    "    'n_estimators': [175, 200, 250, 300],\n",
    "    'max_depth': [10, 15, 20, 25, 30],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'subsample': [0.8, 0.9, 1.0]\n",
    "}\n",
    "\n",
    "# Grid search with cross-validation\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5, n_jobs=-1, verbose=2, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best model\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Best Parameters: {'learning_rate': 0.05, 'max_depth': 15, 'n_estimators': 200, 'subsample': 0.9}\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "\n",
    "# Feature importance from the best model\n",
    "feature_importances = best_model.feature_importances_\n",
    "feature_names = X.columns\n",
    "feature_importances_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importances})\n",
    "print(feature_importances_df.sort_values(by='Importance', ascending=False))\n",
    "\n",
    "# Predictions and accuracy\n",
    "predictions = best_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(f'Accuracy: {accuracy}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-11T14:39:59.187700Z",
     "start_time": "2023-12-11T14:38:48.531296Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "MsT3dcjHXnpk",
    "ExecuteTime": {
     "end_time": "2023-12-11T14:36:11.926060Z",
     "start_time": "2023-12-11T14:36:11.900166Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "new_data = pd.read_csv('binary_classification_datasets/test.csv')\n",
    "new_data.fillna(new_data.mean(), inplace=True)\n",
    "new_data_selected_features = new_data[important_features + ['Id']]\n",
    "\n",
    "new_data_scaled = scaler.transform(new_data_selected_features.drop('Id', axis=1))\n",
    "new_predictions = best_model.predict(new_data_scaled)\n",
    "\n",
    "submission = pd.DataFrame({'target': new_predictions, 'Id': new_data_selected_features['Id']})\n",
    "submission.to_csv('binary_classification_datasets/final_submission.csv', index=False) # acc = 0.942\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Theoretically, a slightly better version \n",
    "Selecting the model that will have the best accuracy on various number of features. \n",
    "Acc = 0.92, with refined grid 0.918"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 720 candidates, totalling 3600 fits\n",
      "Best Parameters: {'learning_rate': 0.07, 'max_depth': 8, 'n_estimators': 175, 'subsample': 0.85}\n",
      "Using 13 features, Accuracy: 0.905\n",
      "Fitting 5 folds for each of 720 candidates, totalling 3600 fits\n",
      "Best Parameters: {'learning_rate': 0.05, 'max_depth': 8, 'n_estimators': 225, 'subsample': 0.95}\n",
      "Using 12 features, Accuracy: 0.92\n",
      "Fitting 5 folds for each of 720 candidates, totalling 3600 fits\n",
      "Best Parameters: {'learning_rate': 0.07, 'max_depth': 10, 'n_estimators': 175, 'subsample': 0.9}\n",
      "Using 11 features, Accuracy: 0.91\n",
      "Fitting 5 folds for each of 720 candidates, totalling 3600 fits\n",
      "Best Parameters: {'learning_rate': 0.07, 'max_depth': 10, 'n_estimators': 200, 'subsample': 1.0}\n",
      "Using 10 features, Accuracy: 0.92\n",
      "Fitting 5 folds for each of 720 candidates, totalling 3600 fits\n",
      "Best Parameters: {'learning_rate': 0.05, 'max_depth': 10, 'n_estimators': 275, 'subsample': 1.0}\n",
      "Using 9 features, Accuracy: 0.905\n",
      "Fitting 5 folds for each of 720 candidates, totalling 3600 fits\n",
      "Best Parameters: {'learning_rate': 0.03, 'max_depth': 10, 'n_estimators': 175, 'subsample': 0.85}\n",
      "Using 8 features, Accuracy: 0.91\n",
      "Best feature set: ['feature1', 'feature3', 'feature4', 'feature5', 'feature6', 'feature7', 'feature8', 'feature9', 'feature10', 'feature11', 'feature12', 'feature13']\n",
      "Best accuracy: 0.92\n",
      "Model with best accuracy stored in variable 'best_model'\n"
     ]
    }
   ],
   "source": [
    "# Read and preprocess the data\n",
    "data = pd.read_csv('binary_classification_datasets/train.csv')\n",
    "data.fillna(data.mean(), inplace=True)\n",
    "\n",
    "# Start with all features\n",
    "all_features = [f'feature{i}' for i in range(1, 14)]\n",
    "best_features = all_features.copy()\n",
    "X = data[all_features]\n",
    "y = data['target']\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=30)\n",
    "\n",
    "# Feature scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "best_accuracy = 0\n",
    "best_feature_set = best_features.copy()\n",
    "best_model = None\n",
    "\n",
    "# Function to perform grid search and return the best model in grid and its accuracy\n",
    "def perform_grid_search(X_train_grid, X_test_grid, y_train_grid, y_test_grid):\n",
    "    model_tmp = xgb.XGBClassifier()\n",
    "    param_grid_tmp = {\n",
    "        'n_estimators': [150, 200, 250, 300],\n",
    "        'max_depth': [5, 10, 15, 20, 25],\n",
    "        'learning_rate': [0.01, 0.05, 0.1, 0.15],\n",
    "        'subsample': [0.8, 0.9, 1.0]\n",
    "    }\n",
    "#     Best Parameters: {'learning_rate': 0.05, 'max_depth': 15, 'n_estimators': 200, 'subsample': 0.9}\n",
    "# Using 13 features, Accuracy: 0.91\n",
    "# Best Parameters: {'learning_rate': 0.05, 'max_depth': 15, 'n_estimators': 300, 'subsample': 0.9}\n",
    "# Using 12 features, Accuracy: 0.905\n",
    "# Best Parameters: {'learning_rate': 0.1, 'max_depth': 10, 'n_estimators': 200, 'subsample': 0.9}\n",
    "# Using 11 features, Accuracy: 0.93\n",
    "# Best Parameters: {'learning_rate': 0.05, 'max_depth': 15, 'n_estimators': 200, 'subsample': 1.0}\n",
    "# Using 10 features, Accuracy: 0.915\n",
    "# Best Parameters: {'learning_rate': 0.05, 'max_depth': 10, 'n_estimators': 300, 'subsample': 1.0}\n",
    "# Using 9 features, Accuracy: 0.905\n",
    "# Best Parameters: {'learning_rate': 0.05, 'max_depth': 15, 'n_estimators': 200, 'subsample': 0.8}\n",
    "# Using 8 features, Accuracy: 0.91\n",
    "\n",
    "    # param_grid_refined = {\n",
    "    # 'n_estimators': [175, 200, 225, 250, 275, 300],\n",
    "    # 'max_depth': [8, 10, 12, 15, 18, 20],\n",
    "    # 'learning_rate': [0.03, 0.05, 0.07, 0.1],\n",
    "    # 'subsample': [0.8, 0.85, 0.9, 0.95, 1.0]\n",
    "    # }\n",
    "    # param_grid_tmp = param_grid_refined\n",
    "    grid_search_tmp = GridSearchCV(model_tmp, param_grid_tmp, cv=5, n_jobs=-1, verbose=1, scoring='accuracy')\n",
    "    grid_search_tmp.fit(X_train_grid, y_train_grid)\n",
    "    \n",
    "    best_grid_model = grid_search_tmp.best_estimator_\n",
    "    predictions_tmp = best_grid_model.predict(X_test_grid)\n",
    "    test_accuracy_tmp = accuracy_score(y_test_grid, predictions_tmp) \n",
    "    \n",
    "    print(\"Best Parameters:\", grid_search_tmp.best_params_)\n",
    "    return best_grid_model, test_accuracy_tmp\n",
    "\n",
    "# Iteratively remove the least important feature\n",
    "# With less than 8 features -- much worse accuracy\n",
    "while len(all_features) > 7:\n",
    "    # Train and evaluate model\n",
    "    current_model, accuracy = perform_grid_search(X_train_scaled, X_test_scaled, y_train, y_test)\n",
    "    print(f\"Using {len(all_features)} features, Accuracy: {accuracy}\")\n",
    "\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        best_feature_set = all_features.copy()\n",
    "        best_model = current_model\n",
    "\n",
    "    # Calculate feature importance and remove the least important feature\n",
    "    feature_importances = current_model.feature_importances_\n",
    "    least_important_feature = all_features[feature_importances.argmin()]\n",
    "    all_features.remove(least_important_feature)\n",
    "    \n",
    "    # Update training and testing sets\n",
    "    X_train_scaled = scaler.fit_transform(X_train[all_features])\n",
    "    X_test_scaled = scaler.transform(X_test[all_features])\n",
    "\n",
    "# Output the best feature set and corresponding accuracy\n",
    "print(\"Best feature set:\", best_feature_set)\n",
    "print(\"Best accuracy:\", best_accuracy)\n",
    "print(\"Model with best accuracy stored in variable 'best_model'\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-11T14:17:03.074873Z",
     "start_time": "2023-12-11T13:58:51.636706Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# Read and preprocess the test data\n",
    "new_data = pd.read_csv('binary_classification_datasets/test.csv')\n",
    "new_data.fillna(new_data.mean(), inplace=True)\n",
    "\n",
    "# Select the same features that were selected in the final model\n",
    "selected_features = best_feature_set\n",
    "new_data_selected_features = new_data[selected_features + ['Id']]\n",
    "\n",
    "new_data_scaled = scaler.fit_transform(new_data_selected_features.drop('Id', axis=1))\n",
    "new_predictions = best_model.predict(new_data_scaled)\n",
    "\n",
    "# Prepare the submission file\n",
    "submission = pd.DataFrame({'Id': new_data_selected_features['Id'], 'target': new_predictions})\n",
    "FILENAME = 'submission2_refined_grid'\n",
    "submission.to_csv(f'binary_classification_datasets/{FILENAME}.csv', index=False)\n",
    "print(\"done\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-11T14:17:57.021893Z",
     "start_time": "2023-12-11T14:17:56.997873Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNab5ej0QbsNu453aILn+OV",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
