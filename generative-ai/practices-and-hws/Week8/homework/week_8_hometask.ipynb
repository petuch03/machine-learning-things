{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "d40c1cb6d7d1478380009119bd32df1a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_aca69185c3d64c4c973436cf17aed8b6",
              "IPY_MODEL_9139ef22acd14db68d5525014650e5a0",
              "IPY_MODEL_c5ed2e57bf7443fbba27192c865e2d82"
            ],
            "layout": "IPY_MODEL_fb9586a9deb6404ba118dc0e713c49b4"
          }
        },
        "aca69185c3d64c4c973436cf17aed8b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8b3125fdfaee4cbdaf082526ead5eeeb",
            "placeholder": "​",
            "style": "IPY_MODEL_afdfd4deab41406cac05cc16ff005b6b",
            "value": "model.safetensors: 100%"
          }
        },
        "9139ef22acd14db68d5525014650e5a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2199523da8f14922b6a5b3cd85466b01",
            "max": 5590927496,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6a49a0c3a72f4bac896a02330d53a1b7",
            "value": 5590927496
          }
        },
        "c5ed2e57bf7443fbba27192c865e2d82": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6ebde9046bf94b0c8265844c226932fa",
            "placeholder": "​",
            "style": "IPY_MODEL_7dd831e62581457a9f36e4ea2b318190",
            "value": " 5.59G/5.59G [01:06&lt;00:00, 118MB/s]"
          }
        },
        "fb9586a9deb6404ba118dc0e713c49b4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8b3125fdfaee4cbdaf082526ead5eeeb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "afdfd4deab41406cac05cc16ff005b6b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2199523da8f14922b6a5b3cd85466b01": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6a49a0c3a72f4bac896a02330d53a1b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6ebde9046bf94b0c8265844c226932fa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7dd831e62581457a9f36e4ea2b318190": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cf95845d554f4314b57202b8239a86f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_029be1a833344db5a6dfdbec3e53e714",
              "IPY_MODEL_0f02bec117944120beeb96d36bbf965f",
              "IPY_MODEL_16f28a4de22f4b229ae28993dc6967f2"
            ],
            "layout": "IPY_MODEL_d30c34211bf24f549ab16674532ea07a"
          }
        },
        "029be1a833344db5a6dfdbec3e53e714": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9bbfee1650f14b86932913ae7b6f39a1",
            "placeholder": "​",
            "style": "IPY_MODEL_c7e21556d02f407390716cb1438b2b64",
            "value": "generation_config.json: 100%"
          }
        },
        "0f02bec117944120beeb96d36bbf965f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eb6365057bd542e18f527746a4b8b4e3",
            "max": 111,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d63ce61d8e364514bb8da5cd7aa22cd8",
            "value": 111
          }
        },
        "16f28a4de22f4b229ae28993dc6967f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_76ff791ef800457bba19894ea0286b53",
            "placeholder": "​",
            "style": "IPY_MODEL_5b533f9ab7844e3083b1a95de65f0b64",
            "value": " 111/111 [00:00&lt;00:00, 5.58kB/s]"
          }
        },
        "d30c34211bf24f549ab16674532ea07a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9bbfee1650f14b86932913ae7b6f39a1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c7e21556d02f407390716cb1438b2b64": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "eb6365057bd542e18f527746a4b8b4e3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d63ce61d8e364514bb8da5cd7aa22cd8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "76ff791ef800457bba19894ea0286b53": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5b533f9ab7844e3083b1a95de65f0b64": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Task 1. Interpretation with Logit Lens\n",
        "\n",
        "Logit Lens is an interpretation technique introduced in [this post](https://www.lesswrong.com/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens). The idea is the following. Imagine that we predict the continuation of a phrase \"IPhone was developed by\". We naturally expect to see \"Apple\", but we're also curious to see the \"thought process\" of an LLM, so we **feed outputs of intermediate layers (intermediate transformer blocks) to the classification head** to see *what would an LLM output if we cut its \"thought process\" short in the middle of it*. The general trend, as one moves from earlier to later layers, is\n",
        "- \"nonsense / not interpretable\" (sometimes, in very early layers) -->\n",
        "  - \"shallow guesses\" (words that are the right part of speech / register / etc) -->\n",
        "- \"better guesses\" near the end.\n",
        "However, it's not always like that, of course.\n",
        "\n",
        "The author of the Logit Lens also created visualization tools and published a [jupyter notebook demo](https://colab.research.google.com/drive/1MjdfK2srcerLrAJDRaJQKO0sUiZ-hQtA?usp=sharing) with cool pictures, but in this task you'll need to reproduce the Logit Lens technique on your own.\n",
        "\n",
        "**Note** If you're really short on compute, you can use GPT-2 instead of zephyr, but you risk losing all the fun and most of interpretability."
      ],
      "metadata": {
        "id": "HipjxMgiDZ4z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 1.1. (1 point)** Write a function\n",
        "\n",
        "```\n",
        "logit_lens(model, input_sentence, top_k)\n",
        "```\n",
        "\n",
        "that for each transformer block returns a dictionary\n",
        "\n",
        "```\n",
        "{\n",
        "    'top_tokens' : [\n",
        "        sorted list of top_k tokens,\n",
        "        from most probable to least probable,\n",
        "        according to the classification head\n",
        "        ],\n",
        "    'top_token_logits' : [logits of these tokens]\n",
        "}\n",
        "```\n",
        "\n",
        "Hint:\n",
        "- To get hidden states of a model you'll need to use `model(**encoded_input, output_hidden_states=True)` instead of `model.generate`\n"
      ],
      "metadata": {
        "id": "D23vGULEFQsU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is how it should work:"
      ],
      "metadata": {
        "id": "MtqnfCh2IYwv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers\n",
        "!pip install -q accelerate"
      ],
      "metadata": {
        "id": "llVZfior0Lwf"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained('stabilityai/stablelm-zephyr-3b')\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    'stabilityai/stablelm-zephyr-3b',\n",
        "    trust_remote_code=True,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "prompt = [{'role': 'user', 'content': 'List 3 synonyms for the word \"tiny\"'}]\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "    prompt,\n",
        "    add_generation_prompt=True,\n",
        "    return_tensors='pt'\n",
        ")\n",
        "\n",
        "tokens = model.generate(\n",
        "    inputs.to(model.device),\n",
        "    max_new_tokens=1024,\n",
        "    temperature=0.8,\n",
        "    do_sample=True\n",
        ")\n",
        "\n",
        "print(tokenizer.decode(tokens[0], skip_special_tokens=False))"
      ],
      "metadata": {
        "id": "JafFNnijBQ4H",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356,
          "referenced_widgets": [
            "d40c1cb6d7d1478380009119bd32df1a",
            "aca69185c3d64c4c973436cf17aed8b6",
            "9139ef22acd14db68d5525014650e5a0",
            "c5ed2e57bf7443fbba27192c865e2d82",
            "fb9586a9deb6404ba118dc0e713c49b4",
            "8b3125fdfaee4cbdaf082526ead5eeeb",
            "afdfd4deab41406cac05cc16ff005b6b",
            "2199523da8f14922b6a5b3cd85466b01",
            "6a49a0c3a72f4bac896a02330d53a1b7",
            "6ebde9046bf94b0c8265844c226932fa",
            "7dd831e62581457a9f36e4ea2b318190",
            "cf95845d554f4314b57202b8239a86f8",
            "029be1a833344db5a6dfdbec3e53e714",
            "0f02bec117944120beeb96d36bbf965f",
            "16f28a4de22f4b229ae28993dc6967f2",
            "d30c34211bf24f549ab16674532ea07a",
            "9bbfee1650f14b86932913ae7b6f39a1",
            "c7e21556d02f407390716cb1438b2b64",
            "eb6365057bd542e18f527746a4b8b4e3",
            "d63ce61d8e364514bb8da5cd7aa22cd8",
            "76ff791ef800457bba19894ea0286b53",
            "5b533f9ab7844e3083b1a95de65f0b64"
          ]
        },
        "outputId": "5e87a69d-3bea-4ca7-f33a-b42f445b5ffb"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/5.59G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d40c1cb6d7d1478380009119bd32df1a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cf95845d554f4314b57202b8239a86f8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<|user|>\n",
            "List 3 synonyms for the word \"tiny\"<|endoftext|>\n",
            "<|assistant|>\n",
            "1. A wee \n",
            "2. A miniature \n",
            "3. Petite<|endoftext|>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "def logit_lens(model, input_sentence, top_k=5):\n",
        "    model = model.eval()\n",
        "\n",
        "    tokenized_sequence_ids = tokenizer.encode(input_sentence, return_tensors=\"pt\")\n",
        "    hidden_states = model(tokenized_sequence_ids, output_hidden_states=True).hidden_states\n",
        "\n",
        "    result_list = list()\n",
        "    for i, hidden_state in enumerate(hidden_states):\n",
        "        logits = model.lm_head(hidden_state)[:, -1, :]\n",
        "        top_k_values, top_k_indices = torch.topk(logits, top_k)\n",
        "        indices = top_k_indices.flatten().detach().numpy()\n",
        "        tokens = tokenizer.convert_ids_to_tokens(indices)\n",
        "        dict_tokens = [\" \" + str(token[1:]) for token in tokens]\n",
        "        dict_values = top_k_values.flatten().detach().numpy()\n",
        "        dict_element = {\n",
        "            'top_tokens': dict_tokens,\n",
        "            'top_tokens_logits': dict_values\n",
        "        }\n",
        "        result_list.append(dict_element)\n",
        "    return result_list\n",
        "\n"
      ],
      "metadata": {
        "id": "y-15EHRPGvFN"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = logit_lens(model, \"IPhone was developed by\", top_k=5)"
      ],
      "metadata": {
        "id": "A0-j00fhH4wP"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result[-2:]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0_VuCRHTyobc",
        "outputId": "2efb19df-1046-46ff-d981-25b1f36b82a5"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'top_tokens': [' Apple', ' a', ' AT', ' ', ' S'],\n",
              "  'top_tokens_logits': array([7.0456634, 6.7790318, 6.1888924, 5.987253 , 5.8684807],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' Apple', ' a', ' the', ' IBM', ' an'],\n",
              "  'top_tokens_logits': array([21.439018, 18.034657, 15.898301, 15.69769 , 15.678806],\n",
              "        dtype=float32)}]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you see, \"Apple\" appears as the most probable token in the last two layers."
      ],
      "metadata": {
        "id": "7rwbGrocR2CG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 1.2 (2 points)**\n",
        "\n",
        "Now you'll use Logit Lens to investigate how transformers deal with redefinition.\n",
        "\n",
        "Use Logit Lens on the sentence\n",
        "\n",
        "```\n",
        "\"In this text the word IPhone means Windows operating system. IPhone was developed by\"\n",
        "```\n",
        "\n",
        "Look at the most probable tokens for all layers. A good LLM knows that IPhone was developed by Apple through *memorization*. However, *in-context learning* will press it to output Microsoft. Check in which layers the most probable token is Microsoft and in which it is Apple.\n",
        "\n",
        "Perform experiments with 9 other sentences with redefinition. Can you observe a pattern of competition between memorization and in-context learning?"
      ],
      "metadata": {
        "id": "4AkMOVa4SQDv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = logit_lens(model, \"In this text the word IPhone means Windows operating system. IPhone was developed by\", top_k=5)\n",
        "result[:]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rqzoIzXx45ap",
        "outputId": "e142c7eb-79b3-478e-b8e5-f6221ee385e0"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'top_tokens': [' means', ' way', ' virtue', ' Means', ' ĩ'],\n",
              "  'top_tokens_logits': array([0.03197266, 0.02827449, 0.02588894, 0.02294105, 0.02270618],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' [...]', ' means', ' the', ' way', ' Ľ'],\n",
              "  'top_tokens_logits': array([0.06112102, 0.05731745, 0.0570225 , 0.05604238, 0.05276864],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' means', ' way', ' Ľ', ' one', ' the'],\n",
              "  'top_tokens_logits': array([0.08996621, 0.0829972 , 0.07946736, 0.07795769, 0.07659164],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' means', ' the', ' eans', ' inf', ' divers'],\n",
              "  'top_tokens_logits': array([0.11120468, 0.09633729, 0.09613051, 0.09183606, 0.08905748],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' pan', ' ES', ' the', ' means', ' Epstein'],\n",
              "  'top_tokens_logits': array([0.18458788, 0.16906188, 0.16186722, 0.16114132, 0.15615094],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' pan', ' I', ' ez', ' Prof', ' Jesse'],\n",
              "  'top_tokens_logits': array([0.22438748, 0.21001042, 0.20969096, 0.20882729, 0.20619345],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' ez', ' eams', ' pro', ' L', ' Prof'],\n",
              "  'top_tokens_logits': array([0.26281384, 0.24426043, 0.24016306, 0.2393946 , 0.2241689 ],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' Cs', ' Wheeler', ' land', ' D', ' rojects'],\n",
              "  'top_tokens_logits': array([0.27432635, 0.26835275, 0.26608023, 0.26269642, 0.26251066],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' one', ' Prof', ' Oliver', ' eams', ' D'],\n",
              "  'top_tokens_logits': array([0.37225586, 0.34413308, 0.3348636 , 0.33277354, 0.3211793 ],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' rant', ' lug', ' one', ' D', ' a'],\n",
              "  'top_tokens_logits': array([0.3847426 , 0.3791182 , 0.37680703, 0.36533475, 0.35983518],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' D', ' u', ' ecogn', ' lug', ' plug'],\n",
              "  'top_tokens_logits': array([0.469876  , 0.469202  , 0.41750336, 0.41413182, 0.40277305],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' ek', ' ut', ' D', ' lug', ' s'],\n",
              "  'top_tokens_logits': array([0.50842136, 0.45976874, 0.43729842, 0.4319868 , 0.42894655],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' sh', ' privately', ' ek', ' u', ' one'],\n",
              "  'top_tokens_logits': array([0.59077597, 0.52743846, 0.5087971 , 0.486017  , 0.48159847],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' one', ' or', ' recognised', ' lis', ' sh'],\n",
              "  'top_tokens_logits': array([0.53463626, 0.5075724 , 0.50385654, 0.5024284 , 0.49687612],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' âĢĭ', ' or', ' virtue', ' contrast', ' either'],\n",
              "  'top_tokens_logits': array([0.568253  , 0.5663849 , 0.55904686, 0.5562837 , 0.52659637],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' swell', ' closely', ' hopeful', ' none', ' virtue'],\n",
              "  'top_tokens_logits': array([0.67821217, 0.67064685, 0.65314263, 0.6435769 , 0.6320915 ],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' virtue', ' companies', ' engineers', ' closely', ' outs'],\n",
              "  'top_tokens_logits': array([0.91392595, 0.81395376, 0.78234977, 0.7456843 , 0.74281585],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' outs', ' leading', ' highly', ' teams', ' closely'],\n",
              "  'top_tokens_logits': array([0.957993  , 0.89883935, 0.8787104 , 0.8655673 , 0.8557564 ],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' Apple', ' hype', ' companies', ' outs', ' means'],\n",
              "  'top_tokens_logits': array([1.1937869, 1.1462113, 1.073266 , 1.0679883, 1.0644958],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' scratch', ' default', ' techn', ' formally', ' Nokia'],\n",
              "  'top_tokens_logits': array([1.1720717, 1.1572845, 1.1418719, 1.1373475, 1.1345686],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' developers',\n",
              "   ' companies',\n",
              "   ' wave',\n",
              "   ' development',\n",
              "   ' formally'],\n",
              "  'top_tokens_logits': array([1.4977033, 1.3494636, 1.3357434, 1.3258278, 1.2980379],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' developers',\n",
              "   ' wave',\n",
              "   ' scratch',\n",
              "   ' companies',\n",
              "   ' development'],\n",
              "  'top_tokens_logits': array([1.5106114, 1.49817  , 1.4834635, 1.4664104, 1.4189596],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' Apple',\n",
              "   ' developers',\n",
              "   ' Microsoft',\n",
              "   ' development',\n",
              "   ' someone'],\n",
              "  'top_tokens_logits': array([2.3009615, 2.0163915, 1.9629792, 1.8212733, 1.7094014],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' Apple', ' Microsoft', ' apple', ' developers', ' pple'],\n",
              "  'top_tokens_logits': array([3.1569526, 2.8678684, 2.34493  , 2.2740884, 2.2320588],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' Microsoft', ' Apple', ' icrosoft', ' developers', ' pple'],\n",
              "  'top_tokens_logits': array([3.8289905, 3.618133 , 2.9511254, 2.761131 , 2.673122 ],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' Apple', ' Microsoft', ' pple', ' icrosoft', ' apple'],\n",
              "  'top_tokens_logits': array([4.769258 , 4.4811172, 3.598158 , 3.4732184, 3.3909364],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' Apple', ' Microsoft', ' pple', ' apple', ' icrosoft'],\n",
              "  'top_tokens_logits': array([6.318704 , 5.853615 , 4.938522 , 4.8020487, 4.646761 ],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' Apple', ' Microsoft', ' pple', ' apple', ' icrosoft'],\n",
              "  'top_tokens_logits': array([7.264104 , 6.6270137, 5.6753144, 5.542102 , 5.242394 ],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' Apple', ' Microsoft', ' pple', ' apple', ' Nokia'],\n",
              "  'top_tokens_logits': array([8.027528 , 7.1730947, 6.5034704, 6.2458625, 5.906921 ],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' Apple', ' Microsoft', ' pple', ' apple', ' Nokia'],\n",
              "  'top_tokens_logits': array([7.9606776, 6.467429 , 6.379094 , 6.2220306, 5.2265806],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' Apple', ' Microsoft', ' pple', ' apple', ' Nokia'],\n",
              "  'top_tokens_logits': array([6.959031 , 6.729102 , 5.2466626, 5.168402 , 4.901674 ],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' Microsoft', ' Apple', ' a', ' MS', ' AT'],\n",
              "  'top_tokens_logits': array([7.3243456, 7.214468 , 6.0353374, 5.597833 , 5.4640493],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' Microsoft', ' Apple', ' a', ' Nokia', ' IBM'],\n",
              "  'top_tokens_logits': array([21.186926, 20.47797 , 18.12183 , 15.97065 , 14.94632 ],\n",
              "        dtype=float32)}]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = logit_lens(model, \"In this text, the word 'coffee' means 'solar energy.' The world's largest coffee plant is located in\", top_k=5)\n",
        "result[:]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oHlmlQhIUDMi",
        "outputId": "e2ce72e1-baae-4ac9-9789-9a85030b333a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'top_tokens': [' cribed', ' accordance', ' criptions', ' vitro', ' spite'],\n",
              "  'top_tokens_logits': array([0.03589612, 0.02858505, 0.02758173, 0.02661785, 0.0255783 ],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' ffect', ' [...]', ' ', ' trop', ' the'],\n",
              "  'top_tokens_logits': array([0.06045042, 0.05886982, 0.05778028, 0.05444053, 0.05206032],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' xperienced', ' trop', ' ul', ' cribed', ' ffect'],\n",
              "  'top_tokens_logits': array([0.09249139, 0.0907546 , 0.08961549, 0.08892887, 0.08602268],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' xperienced', ' ffect', ' cribed', ' ul', ' Syracuse'],\n",
              "  'top_tokens_logits': array([0.1278651 , 0.11917061, 0.11349771, 0.11069141, 0.10752887],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' µÑģÑĤ', ' Ľ', ' duc', ' br', ' ©nÃ©'],\n",
              "  'top_tokens_logits': array([0.19282682, 0.1927225 , 0.18679161, 0.18534286, 0.17976344],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' p', ' pp', ' Spin', ' spin', ' ©nÃ©'],\n",
              "  'top_tokens_logits': array([0.2154269 , 0.21518585, 0.21515779, 0.21252815, 0.2096245 ],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' nowhere', ' Cs', ' uver', ' shi', ' os'],\n",
              "  'top_tokens_logits': array([0.27007827, 0.24924725, 0.24546768, 0.24292898, 0.23923352],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' ©nÃ©', ' pin', ' spin', ' Spin', ' ae'],\n",
              "  'top_tokens_logits': array([0.3225109 , 0.29423922, 0.2743638 , 0.27301055, 0.27161208],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' ©nÃ©', ' hopes', ' p', ' southeast', ' P'],\n",
              "  'top_tokens_logits': array([0.36507726, 0.35158136, 0.34544647, 0.34126207, 0.32229212],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' southeast', ' farms', ' regions', ' farming', ' Farm'],\n",
              "  'top_tokens_logits': array([0.46861538, 0.4666739 , 0.45809856, 0.45665812, 0.4478623 ],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' southeast', ' re', ' northeast', ' farms', ' southwest'],\n",
              "  'top_tokens_logits': array([0.4545163 , 0.44337028, 0.4296128 , 0.4227211 , 0.42245054],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' cooperation',\n",
              "   ' southwest',\n",
              "   ' nowhere',\n",
              "   ' halls',\n",
              "   ' cooperative'],\n",
              "  'top_tokens_logits': array([0.5426677 , 0.52251667, 0.51604444, 0.50917816, 0.50309706],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' cooperation', ' halls', ' bb', ' locations', ' Pacific'],\n",
              "  'top_tokens_logits': array([0.5409996 , 0.52975893, 0.52439135, 0.5187275 , 0.51439047],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' cooperation', ' Denmark', ' China', ' North', ' nowhere'],\n",
              "  'top_tokens_logits': array([0.72999644, 0.6965676 , 0.68253803, 0.6400822 , 0.61247987],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' cooperation', ' central', ' democratic', ' Ma', ' Denmark'],\n",
              "  'top_tokens_logits': array([0.70165837, 0.67822945, 0.6428169 , 0.6417242 , 0.6281397 ],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' Mexico', ' Asia', ' Japan', ' China', ' Peru'],\n",
              "  'top_tokens_logits': array([0.7985736 , 0.7888074 , 0.7832834 , 0.7779771 , 0.76091254],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' Mexico', ' China', ' Japan', ' nd', ' nowhere'],\n",
              "  'top_tokens_logits': array([0.99101853, 0.90184075, 0.88053524, 0.8788475 , 0.8718489 ],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' Mexico', ' Brazil', ' Japan', ' eastern', ' Asia'],\n",
              "  'top_tokens_logits': array([1.1296731, 1.086528 , 1.0314994, 1.0244284, 1.005048 ],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' Brazil', ' Florida', ' Mexico', ' India', ' Asia'],\n",
              "  'top_tokens_logits': array([1.2939403, 1.2735674, 1.1947603, 1.1066693, 1.0918698],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' Brazil', ' hopes', ' Mexico', ' Florida', ' India'],\n",
              "  'top_tokens_logits': array([1.4817513, 1.3334357, 1.2465968, 1.2154524, 1.1759297],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' Brazil', ' central', ' hopes', ' Mexico', ' Florida'],\n",
              "  'top_tokens_logits': array([1.7419662, 1.5191526, 1.453123 , 1.4513023, 1.447399 ],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' Brazil', ' central', ' Florida', ' Mexico', ' Guatemala'],\n",
              "  'top_tokens_logits': array([2.3528073, 1.9595654, 1.9029021, 1.8949568, 1.8034785],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' Brazil', ' Mexico', ' Florida', ' Guatemala', ' central'],\n",
              "  'top_tokens_logits': array([2.8332562, 2.2819896, 2.194076 , 2.150747 , 2.036147 ],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' Brazil', ' Guatemala', ' Mexico', ' Florida', ' Colombia'],\n",
              "  'top_tokens_logits': array([3.8693643, 3.0515785, 3.0140624, 2.8395238, 2.8297415],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' Brazil', ' Guatemala', ' Florida', ' Colombia', ' Mexico'],\n",
              "  'top_tokens_logits': array([4.2399473, 3.389077 , 3.259403 , 3.2111406, 3.1381993],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' Brazil', ' Guatemala', ' Colombia', ' Costa', ' Florida'],\n",
              "  'top_tokens_logits': array([5.1819544, 3.99502  , 3.743786 , 3.6655478, 3.6288004],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' Brazil', ' Guatemala', ' Colombia', ' Costa', ' Mexico'],\n",
              "  'top_tokens_logits': array([6.031921 , 4.582031 , 4.5138464, 4.164261 , 4.104906 ],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' Brazil', ' Guatemala', ' Colombia', ' Hond', ' Hawaii'],\n",
              "  'top_tokens_logits': array([6.462427 , 4.85957  , 4.8041954, 4.527505 , 4.4680133],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' Brazil', ' Guatemala', ' Colombia', ' Hawaii', ' Ecuador'],\n",
              "  'top_tokens_logits': array([6.976258 , 5.679825 , 5.415283 , 4.8769045, 4.8718324],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' Brazil', ' Guatemala', ' Costa', ' Colombia', ' Central'],\n",
              "  'top_tokens_logits': array([6.8776307, 5.6119604, 5.425559 , 5.126121 , 5.0938506],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' Brazil', ' Central', ' Costa', ' U', ' Hawaii'],\n",
              "  'top_tokens_logits': array([6.4777675, 5.99714  , 5.124866 , 5.105049 , 5.099452 ],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' Central', ' Brazil', ' U', ' Bel', ' Hawaii'],\n",
              "  'top_tokens_logits': array([6.3639812, 6.2281623, 6.179239 , 5.966852 , 5.4257584],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' Brazil', ' Hawaii', ' Colombia', ' Costa', ' Guatemala'],\n",
              "  'top_tokens_logits': array([21.416111, 19.187891, 17.46339 , 17.135723, 16.256096],\n",
              "        dtype=float32)}]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = logit_lens(model, \"In this document, 'bicycle' refers to 'the Internet.' The invention of the bicycle drastically changed\", top_k=5)\n",
        "result[:]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5PYQ32ReUHNj",
        "outputId": "e0206d71-eb5b-4ee1-9308-9887ea01a64e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'top_tokens': [' ru', ' Gru', ' ecent', ' ouch', ' '],\n",
              "  'top_tokens_logits': array([0.02005338, 0.01981762, 0.01832283, 0.01783607, 0.0174183 ],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' lly', ' all', ' ggest', ' V', ' T'],\n",
              "  'top_tokens_logits': array([0.04918555, 0.04813809, 0.04720778, 0.04681649, 0.04498557],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' ', ' bs', ' Look', ' g', ' ver'],\n",
              "  'top_tokens_logits': array([0.07606998, 0.07529482, 0.07473864, 0.0731097 , 0.07237047],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' L', ' Look', ' pendent', ' fo', ' ol'],\n",
              "  'top_tokens_logits': array([0.10960124, 0.10764145, 0.10713266, 0.10563803, 0.09831663],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' Belf', ' pendent', ' eleton', ' L', ' fo'],\n",
              "  'top_tokens_logits': array([0.14445725, 0.14120461, 0.13766582, 0.13728945, 0.13702376],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' gen', ' knob', ' g', ' (>', ' ul'],\n",
              "  'top_tokens_logits': array([0.29140025, 0.23145452, 0.22231632, 0.22100216, 0.21688838],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' knob', ' ates', ' ract', ' gen', ' perceptions'],\n",
              "  'top_tokens_logits': array([0.29418656, 0.28342718, 0.2653893 , 0.2504638 , 0.2503153 ],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' ates', ' (>', ' I', ' Randall', ' incentives'],\n",
              "  'top_tokens_logits': array([0.3617172 , 0.34580714, 0.30414662, 0.30004618, 0.29360205],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' ates', ' ngton', ' the', ' nthetic', ' Wave'],\n",
              "  'top_tokens_logits': array([0.4229086 , 0.39958417, 0.3515395 , 0.3317454 , 0.33037555],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' ates', ' materially', ' dos', ' nknown', ' eneration'],\n",
              "  'top_tokens_logits': array([0.38492113, 0.35555914, 0.35550445, 0.3549101 , 0.35258567],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' Transform', ' riv', ' ates', ' Wend', ' radically'],\n",
              "  'top_tokens_logits': array([0.43304652, 0.4327081 , 0.42670098, 0.4247629 , 0.42375246],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' radically', ' riv', ' DA', ' ence', ' tion'],\n",
              "  'top_tokens_logits': array([0.5166098 , 0.5159165 , 0.49104452, 0.4848399 , 0.4830424 ],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' eneration', ' Physical', ' vered', ' ng', ' riv'],\n",
              "  'top_tokens_logits': array([0.5364341 , 0.53157187, 0.5252173 , 0.5172789 , 0.51624334],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' ge', ' Physical', ' ngers', ' Later', ' ence'],\n",
              "  'top_tokens_logits': array([0.62324136, 0.61914223, 0.6103145 , 0.57119286, 0.56329346],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' ngers',\n",
              "   ' Physical',\n",
              "   ' Society',\n",
              "   ' radically',\n",
              "   ' eneration'],\n",
              "  'top_tokens_logits': array([0.75762147, 0.74750835, 0.69863725, 0.6939523 , 0.67547834],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' ngers', ' Society', ' society', ' nkind', ' radically'],\n",
              "  'top_tokens_logits': array([0.84169275, 0.8319853 , 0.82043993, 0.79538804, 0.7633136 ],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' society',\n",
              "   ' Society',\n",
              "   ' radically',\n",
              "   ' nkind',\n",
              "   ' revolution'],\n",
              "  'top_tokens_logits': array([1.1010481 , 1.0227059 , 0.9503761 , 0.9136399 , 0.88921225],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' society',\n",
              "   ' Society',\n",
              "   ' radically',\n",
              "   ' humanity',\n",
              "   ' societal'],\n",
              "  'top_tokens_logits': array([1.3529667, 1.2194648, 1.2001328, 1.1297743, 1.0896034],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' society',\n",
              "   ' humanity',\n",
              "   ' radically',\n",
              "   ' Society',\n",
              "   ' societal'],\n",
              "  'top_tokens_logits': array([1.6238127, 1.4192457, 1.4146124, 1.4010373, 1.3940662],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' society', ' radically', ' lives', ' societal', ' Society'],\n",
              "  'top_tokens_logits': array([1.784216 , 1.6697643, 1.6587017, 1.6157632, 1.5016437],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' society',\n",
              "   ' societal',\n",
              "   ' transportation',\n",
              "   ' Society',\n",
              "   ' lives'],\n",
              "  'top_tokens_logits': array([2.1980615, 1.8329804, 1.7829225, 1.7536125, 1.751236 ],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' society', ' human', ' humanity', ' lives', ' societal'],\n",
              "  'top_tokens_logits': array([2.3999043, 2.0422466, 1.9881872, 1.9418147, 1.9264587],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' society',\n",
              "   ' human',\n",
              "   ' transportation',\n",
              "   ' cycling',\n",
              "   ' riding'],\n",
              "  'top_tokens_logits': array([2.916901 , 2.8302422, 2.7957447, 2.4797292, 2.4646707],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' society', ' human', ' transportation', ' cycling', ' life'],\n",
              "  'top_tokens_logits': array([3.2878246, 3.157072 , 3.0076354, 2.8441281, 2.7092319],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' society', ' human', ' transportation', ' mankind', ' life'],\n",
              "  'top_tokens_logits': array([3.9031467, 3.6466522, 3.3326519, 3.1278148, 3.0904615],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' society', ' human', ' life', ' transportation', ' mankind'],\n",
              "  'top_tokens_logits': array([4.653143 , 4.166911 , 3.7003927, 3.6569414, 3.4812198],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' society', ' human', ' life', ' transportation', ' modes'],\n",
              "  'top_tokens_logits': array([5.136932 , 4.72741  , 4.339268 , 4.0725727, 3.960748 ],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' society', ' human', ' life', ' transportation', ' cycling'],\n",
              "  'top_tokens_logits': array([5.2879796, 5.1025424, 4.6174555, 4.3955617, 4.282248 ],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' human', ' society', ' life', ' transportation', ' hum'],\n",
              "  'top_tokens_logits': array([5.7211294, 5.2277627, 4.7102194, 4.5698857, 4.4345675],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' human', ' society', ' hum', ' life', ' physical'],\n",
              "  'top_tokens_logits': array([6.233926 , 5.2540555, 5.17492  , 5.0071225, 4.923152 ],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' human', ' hum', ' the', ' society', ' life'],\n",
              "  'top_tokens_logits': array([7.1166606, 6.099043 , 5.840052 , 5.622932 , 5.5371866],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' human', ' the', ' hum', ' life', \" '\"],\n",
              "  'top_tokens_logits': array([8.364166 , 7.6986895, 7.2692814, 6.5457993, 6.4724183],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' the', ' human', ' transportation', ' our', ' society'],\n",
              "  'top_tokens_logits': array([25.442734, 24.09272 , 22.62041 , 20.884066, 20.880112],\n",
              "        dtype=float32)}]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = logit_lens(model, \"Here, 'chocolate' is used to mean 'artificial intelligence.' Chocolate has significantly evolved over the past decade, especially in\", top_k=5)\n",
        "result[:]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q2YqsV7IUVoc",
        "outputId": "71e53096-81dc-488d-facb-3d7128addede"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'top_tokens': [' cribed', ' accordance', ' criptions', ' vitro', ' spite'],\n",
              "  'top_tokens_logits': array([0.03589612, 0.02858505, 0.02758173, 0.02661785, 0.0255783 ],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' ffect', ' [...]', ' ', ' trop', ' sk'],\n",
              "  'top_tokens_logits': array([0.05948918, 0.05922301, 0.05643798, 0.05490062, 0.05076383],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' cribed', ' xperienced', ' ul', ' trop', ' lusively'],\n",
              "  'top_tokens_logits': array([0.09279542, 0.08920643, 0.0890229 , 0.08867558, 0.0818992 ],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' nden', ' ul', ' uctor', ' emin', ' ersed'],\n",
              "  'top_tokens_logits': array([0.1290568 , 0.12741765, 0.11830679, 0.11261178, 0.10981558],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' ul', ' regards', ' bled', ' uctor', ' nden'],\n",
              "  'top_tokens_logits': array([0.20265877, 0.17241001, 0.16472152, 0.16444688, 0.16435823],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' ette', ' regards', ' ek', ' terms', ' nes'],\n",
              "  'top_tokens_logits': array([0.29018053, 0.2859776 , 0.24521296, 0.24415354, 0.24103694],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' regards', ' ette', ' terms', ' lli', ' ek'],\n",
              "  'top_tokens_logits': array([0.33268964, 0.3044408 , 0.29169244, 0.28884774, 0.28269476],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' terms', ' regards', ' ards', ' smuch', ' erter'],\n",
              "  'top_tokens_logits': array([0.38829628, 0.3761361 , 0.34778038, 0.31298536, 0.3080592 ],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' ards', ' terms', ' hopes', ' regards', ' rast'],\n",
              "  'top_tokens_logits': array([0.43388748, 0.42280126, 0.4086655 , 0.40318495, 0.3914179 ],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' ards', ' ay', ' comparison', ' smuch', ' regards'],\n",
              "  'top_tokens_logits': array([0.5438478 , 0.5239437 , 0.50934285, 0.48184508, 0.47637996],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' regards', ' terms', ' ards', ' regard', ' erter'],\n",
              "  'top_tokens_logits': array([0.6327891 , 0.59554183, 0.54616   , 0.54413486, 0.5172134 ],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' regards', ' terms', ' regard', ' ards', ' rast'],\n",
              "  'top_tokens_logits': array([0.7080128 , 0.63987654, 0.6275834 , 0.5893991 , 0.56572235],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' regards', ' regard', ' terms', ' rast', ' relation'],\n",
              "  'top_tokens_logits': array([0.8168653 , 0.68954533, 0.66923016, 0.6418895 , 0.5565305 ],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' regards', ' regard', ' relation', ' rast', ' ards'],\n",
              "  'top_tokens_logits': array([1.0135965 , 0.84534764, 0.72960246, 0.7279948 , 0.7238506 ],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' regards', ' rast', ' regard', ' divers', ' ards'],\n",
              "  'top_tokens_logits': array([0.8441504, 0.8370041, 0.7116911, 0.6985101, 0.6947447],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' regards', ' rast', ' terms', ' regard', ' relation'],\n",
              "  'top_tokens_logits': array([0.9326153 , 0.8780153 , 0.87213105, 0.86629903, 0.83729374],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' regards', ' relation', ' regard', ' terms', ' rast'],\n",
              "  'top_tokens_logits': array([1.2301317, 1.2172767, 1.1917125, 1.105412 , 1.0535074],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' terms', ' regards', ' regard', ' relation', ' recent'],\n",
              "  'top_tokens_logits': array([1.3757741, 1.3373348, 1.3012167, 1.2018026, 1.1232969],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' relation', ' regards', ' terms', ' recent', ' regard'],\n",
              "  'top_tokens_logits': array([1.6210382, 1.6081991, 1.5847359, 1.4700193, 1.3365557],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' terms', ' relation', ' regards', ' regard', ' recent'],\n",
              "  'top_tokens_logits': array([1.925127 , 1.9148238, 1.8970729, 1.700738 , 1.5494001],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' terms',\n",
              "   ' regards',\n",
              "   ' relation',\n",
              "   ' regard',\n",
              "   ' technological'],\n",
              "  'top_tokens_logits': array([2.3077736, 2.3004417, 2.1442196, 1.9921942, 1.8121672],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' regards', ' terms', ' relation', ' regard', ' Europe'],\n",
              "  'top_tokens_logits': array([2.71347  , 2.7081835, 2.4730556, 2.3879082, 1.9520934],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' terms', ' regards', ' relation', ' regard', ' Europe'],\n",
              "  'top_tokens_logits': array([3.1881042, 2.9568946, 2.9280124, 2.6490915, 2.5192323],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' terms', ' relation', ' regards', ' regard', ' Europe'],\n",
              "  'top_tokens_logits': array([3.9387786, 3.5887663, 3.5882628, 3.331603 , 2.96059  ],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' terms', ' relation', ' regards', ' the', ' recent'],\n",
              "  'top_tokens_logits': array([4.547    , 4.0684705, 3.8937597, 3.873838 , 3.7953806],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' terms', ' relation', ' the', ' regards', ' Europe'],\n",
              "  'top_tokens_logits': array([5.1071544, 4.653271 , 4.5003448, 4.40351  , 4.3258104],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' terms', ' regards', ' relation', ' regard', ' recent'],\n",
              "  'top_tokens_logits': array([7.162935 , 6.4853888, 6.3960066, 5.9024425, 5.271223 ],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' terms', ' regards', ' relation', ' regard', ' recent'],\n",
              "  'top_tokens_logits': array([7.8179064, 6.8755417, 6.8446493, 6.3573127, 5.5004134],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' terms', ' regards', ' relation', ' regard', ' recent'],\n",
              "  'top_tokens_logits': array([7.8764386, 7.2548285, 7.180078 , 6.7629786, 6.1343927],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' terms', ' regards', ' relation', ' regard', ' recent'],\n",
              "  'top_tokens_logits': array([7.776182 , 7.042222 , 6.623008 , 6.4221478, 6.1627054],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' terms', ' regards', ' relation', ' regard', ' the'],\n",
              "  'top_tokens_logits': array([8.237892 , 8.227727 , 7.359556 , 7.3439627, 6.868494 ],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' the', ' terms', ' regards', ' relation', ' recent'],\n",
              "  'top_tokens_logits': array([8.798452 , 8.125036 , 7.9924545, 7.821555 , 7.7562695],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' the', ' terms', ' recent', ' regard', ' regards'],\n",
              "  'top_tokens_logits': array([22.901669, 22.206339, 19.757967, 19.555294, 19.542038],\n",
              "        dtype=float32)}]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = logit_lens(model, \"In this scenario, 'shoes' stand for 'spacecraft.' The most advanced shoes are designed for travel beyond\", top_k=5)\n",
        "result[:]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D8AV2VizUXdc",
        "outputId": "c4a61296-4a96-408b-df9c-ec1087377311"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'top_tokens': [' mes', ' cular', ' n', ' norm', ' yourselves'],\n",
              "  'top_tokens_logits': array([0.01973564, 0.01658559, 0.01656543, 0.01641238, 0.01615398],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' cin', ' am', ' plaus', ' ams', ' ys'],\n",
              "  'top_tokens_logits': array([0.06580061, 0.06410584, 0.0613165 , 0.06072294, 0.05977384],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' es', ' ams', ' mes', ' sper', ' am'],\n",
              "  'top_tokens_logits': array([0.11488405, 0.10880438, 0.10566484, 0.10065822, 0.10010798],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' es', ' ams', ' cate', ' am', ' misses'],\n",
              "  'top_tokens_logits': array([0.13997759, 0.13968557, 0.13698061, 0.13145879, 0.11943168],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' cate', ' mes', ' cha', ' am', ' ams'],\n",
              "  'top_tokens_logits': array([0.18522604, 0.18080069, 0.1706951 , 0.16831215, 0.16750231],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' ©e', ' cate', ' ¾Ð´', ' am', ' [âĢ¦]'],\n",
              "  'top_tokens_logits': array([0.2285909 , 0.21736348, 0.21687883, 0.21409217, 0.2122339 ],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' ary', ' ams', ' tions', ' rds', ' am'],\n",
              "  'top_tokens_logits': array([0.2464713 , 0.24581613, 0.2439155 , 0.24266051, 0.24122776],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' ary', ' ru', ' ranges', ' ams', ' am'],\n",
              "  'top_tokens_logits': array([0.3338588 , 0.31178677, 0.30558857, 0.29960224, 0.2876805 ],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' ', ' ru', ' ary', ' ams', ' nce'],\n",
              "  'top_tokens_logits': array([0.37994888, 0.3732062 , 0.3557091 , 0.35139272, 0.34393248],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' ru', ' elle', ' owered', ' Cru', ' '],\n",
              "  'top_tokens_logits': array([0.42928767, 0.40934792, 0.39580068, 0.39424828, 0.39078936],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' hal', ' ', ' uge', ' f', ' cular'],\n",
              "  'top_tokens_logits': array([0.5846436 , 0.5095326 , 0.49924567, 0.4835987 , 0.4768981 ],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' hal', ' hor', ' ARD', ' ds', ' ards'],\n",
              "  'top_tokens_logits': array([0.6827999 , 0.56990314, 0.5568282 , 0.54974276, 0.5338706 ],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' hal', ' essly', ' ards', ' ARD', ' mn'],\n",
              "  'top_tokens_logits': array([0.6369909 , 0.6052605 , 0.60064286, 0.59749246, 0.55750436],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' hal', ' rds', ' ssen', ' mankind', ' hor'],\n",
              "  'top_tokens_logits': array([0.6894956, 0.6130148, 0.6001964, 0.5811799, 0.5808994],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' Beyond', ' beyond', ' worlds', ' ssen', ' rship'],\n",
              "  'top_tokens_logits': array([0.7048683 , 0.6714313 , 0.65189004, 0.64352804, 0.64281964],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' hal', ' worlds', ' beyond', ' rship', ' Beyond'],\n",
              "  'top_tokens_logits': array([0.76850516, 0.73101056, 0.7049402 , 0.69487715, 0.68762153],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' reaches', ' oceans', ' conventional', ' repro', ' Beyond'],\n",
              "  'top_tokens_logits': array([0.892578  , 0.89252293, 0.8904148 , 0.844777  , 0.8393359 ],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' ea', ' borders', ' oceans', ' conventional', ' mere'],\n",
              "  'top_tokens_logits': array([1.0741767 , 1.0046281 , 0.99644864, 0.98435915, 0.9701235 ],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' conventional', ' Beyond', ' beyond', ' Earth', ' ordinary'],\n",
              "  'top_tokens_logits': array([1.2594097, 1.2315389, 1.1877357, 1.1806333, 1.0942361],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' Earth', ' ntal', ' repro', ' borders', ' ordinary'],\n",
              "  'top_tokens_logits': array([1.3186955, 1.2373447, 1.2333864, 1.2285783, 1.2130564],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' Earth', ' planet', ' planetary', ' space', ' earth'],\n",
              "  'top_tokens_logits': array([2.7863617, 2.4274647, 2.171904 , 2.0672953, 1.92793  ],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' Earth', ' planet', ' planetary', ' human', ' earth'],\n",
              "  'top_tokens_logits': array([2.9852304, 2.5258048, 2.316513 , 2.1909602, 2.1705878],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' Earth', ' earth', ' planet', ' human', ' arth'],\n",
              "  'top_tokens_logits': array([4.2870646, 3.325354 , 3.2659743, 3.2491894, 3.0334308],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' Earth', ' earth', ' human', ' planet', ' arth'],\n",
              "  'top_tokens_logits': array([4.4949465, 3.486112 , 3.3110294, 3.2001204, 3.1673477],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' Earth', ' earth', ' planet', ' arth', ' human'],\n",
              "  'top_tokens_logits': array([5.265568 , 4.000881 , 3.7839084, 3.6317625, 3.4365976],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' Earth', ' earth', ' planet', ' arth', ' human'],\n",
              "  'top_tokens_logits': array([5.9670153, 4.6744637, 4.2362924, 4.077556 , 3.768291 ],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' Earth', ' earth', ' planet', ' arth', ' planetary'],\n",
              "  'top_tokens_logits': array([6.6641665, 5.2014303, 4.91627  , 4.5866632, 4.3655863],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' Earth', ' earth', ' planet', ' Mars', ' arth'],\n",
              "  'top_tokens_logits': array([7.0048523, 5.382238 , 5.1603904, 4.7476087, 4.7381916],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' Earth', ' earth', ' planet', ' Mars', ' arth'],\n",
              "  'top_tokens_logits': array([7.5016694, 5.6849933, 5.3888106, 4.927086 , 4.7596555],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' Earth', ' earth', ' planet', ' Mars', ' arth'],\n",
              "  'top_tokens_logits': array([7.97998  , 5.8885303, 5.559391 , 4.979775 , 4.7525887],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' Earth', ' our', ' earth', ' hum', ' planet'],\n",
              "  'top_tokens_logits': array([7.165985 , 5.9543905, 4.9748287, 4.850209 , 4.8299937],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' Earth', ' our', ' the', \" '\", ' c'],\n",
              "  'top_tokens_logits': array([8.2983465, 7.089506 , 7.057397 , 6.8431864, 6.4174876],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' Earth', ' our', ' the', ' earth', ' planet'],\n",
              "  'top_tokens_logits': array([27.813208, 24.617031, 23.10918 , 21.203785, 20.529432],\n",
              "        dtype=float32)}]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = logit_lens(model, \"For the purpose of this discussion, 'library' means 'quantum computer.' The largest library in the world can process\", top_k=5)\n",
        "result[:]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t7BVMt4jUa0Z",
        "outputId": "63cb1985-8568-4bfa-c5b7-e3823c40afa3"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'top_tokens': [' at', ' ly', ' onal', ' Ĭł', ' kindly'],\n",
              "  'top_tokens_logits': array([0.0205835 , 0.02025671, 0.02006477, 0.01999479, 0.01860488],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' vely', ' rs', ' ly', ' lled', ' sh'],\n",
              "  'top_tokens_logits': array([0.06100681, 0.05752692, 0.05752672, 0.05730348, 0.05436922],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' ons', ' oned', ' ally', ' onal', ' s'],\n",
              "  'top_tokens_logits': array([0.13491163, 0.12558194, 0.11844752, 0.11355867, 0.11017549],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' s', ' ons', ' lam', ' il', ' r'],\n",
              "  'top_tokens_logits': array([0.13920127, 0.13772811, 0.13374054, 0.12912603, 0.12817669],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' ally', ' ³', ' rs', ' lled', ' nto'],\n",
              "  'top_tokens_logits': array([0.19994949, 0.19337416, 0.19044028, 0.1861839 , 0.18003878],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' lled', ' rs', ' lls', ' pp', ' veness'],\n",
              "  'top_tokens_logits': array([0.2684486 , 0.2535708 , 0.25159794, 0.24020875, 0.23497476],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' lls', ' rium', ' veness', ' rial', ' lled'],\n",
              "  'top_tokens_logits': array([0.3256816 , 0.30591983, 0.27754366, 0.2500673 , 0.24986427],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' rium', ' cky', ' Ļ¨', ' lls', ' Ã³'],\n",
              "  'top_tokens_logits': array([0.30924085, 0.30482802, 0.29472584, 0.2933773 , 0.29124397],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' igest', ' digest', ' op', ' Ã³', ' Gonzales'],\n",
              "  'top_tokens_logits': array([0.36470273, 0.35706422, 0.34286132, 0.3417375 , 0.34072512],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' pinal', ' vely', ' NC', ' N', ' Kaw'],\n",
              "  'top_tokens_logits': array([0.41653153, 0.3800237 , 0.37538546, 0.36933517, 0.36833322],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' Hop', ' vely', ' ec', ' rds', ' ccept'],\n",
              "  'top_tokens_logits': array([0.47886965, 0.47327292, 0.4661432 , 0.45697248, 0.44346455],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' calmly', ' Kaw', ' che', ' eeds', ' ec'],\n",
              "  'top_tokens_logits': array([0.47634256, 0.4739391 , 0.47003895, 0.4634267 , 0.45772728],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' billions', ' thousands', ' Kaw', ' gens', ' eading'],\n",
              "  'top_tokens_logits': array([0.6626401 , 0.596007  , 0.59406906, 0.58738464, 0.57808614],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' billions', ' internally', ' thousands', ' 8000', ' Cs'],\n",
              "  'top_tokens_logits': array([0.73730224, 0.7015574 , 0.6726998 , 0.6431019 , 0.61974645],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' billions', ' internally', ' 8000', ' thousands', ' 5000'],\n",
              "  'top_tokens_logits': array([0.75372887, 0.75195754, 0.6899081 , 0.6718753 , 0.6419374 ],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' internally', ' billions', ' thousands', ' speeds', ' 8000'],\n",
              "  'top_tokens_logits': array([0.78160304, 0.75177777, 0.7306169 , 0.73039067, 0.7151117 ],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' internally',\n",
              "   ' billions',\n",
              "   ' giant',\n",
              "   ' thousands',\n",
              "   ' speeds'],\n",
              "  'top_tokens_logits': array([1.081965 , 1.0029789, 0.9599165, 0.913453 , 0.905926 ],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' internally', ' thousands', ' billions', ' giant', ' 5000'],\n",
              "  'top_tokens_logits': array([1.2741517, 1.2033833, 1.1832864, 1.1090133, 1.0215021],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' thousands',\n",
              "   ' billions',\n",
              "   ' giant',\n",
              "   ' simultaneously',\n",
              "   ' internally'],\n",
              "  'top_tokens_logits': array([1.4871626, 1.3758255, 1.3206323, 1.233799 , 1.185453 ],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' thousands',\n",
              "   ' giant',\n",
              "   ' billions',\n",
              "   ' enormous',\n",
              "   ' simultaneously'],\n",
              "  'top_tokens_logits': array([1.6503123, 1.5542314, 1.4760268, 1.462787 , 1.3792126],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' thousands', ' billions', ' giant', ' s', ' enormous'],\n",
              "  'top_tokens_logits': array([1.9288497, 1.7399466, 1.733154 , 1.7316575, 1.7099072],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' thousands',\n",
              "   ' billions',\n",
              "   ' s',\n",
              "   ' simultaneously',\n",
              "   ' millions'],\n",
              "  'top_tokens_logits': array([2.3846066, 2.1117384, 2.098794 , 2.0637817, 2.014361 ],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' thousands',\n",
              "   ' billions',\n",
              "   ' hundreds',\n",
              "   ' millions',\n",
              "   ' enormous'],\n",
              "  'top_tokens_logits': array([2.900881 , 2.5939863, 2.583801 , 2.4606225, 2.3019574],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' thousands',\n",
              "   ' hundreds',\n",
              "   ' millions',\n",
              "   ' billions',\n",
              "   ' enormous'],\n",
              "  'top_tokens_logits': array([3.2803936, 2.9555333, 2.8689022, 2.824827 , 2.7157903],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' thousands',\n",
              "   ' hundreds',\n",
              "   ' billions',\n",
              "   ' millions',\n",
              "   ' enormous'],\n",
              "  'top_tokens_logits': array([3.734091 , 3.5551147, 3.4110057, 3.4094214, 3.308611 ],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' millions',\n",
              "   ' billions',\n",
              "   ' thousands',\n",
              "   ' hundreds',\n",
              "   ' enormous'],\n",
              "  'top_tokens_logits': array([4.9701934, 4.9358625, 4.910415 , 4.448479 , 4.0863066],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' thousands',\n",
              "   ' millions',\n",
              "   ' billions',\n",
              "   ' hundreds',\n",
              "   ' enormous'],\n",
              "  'top_tokens_logits': array([5.39633  , 5.278268 , 5.2307844, 4.934055 , 4.4305906],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' millions',\n",
              "   ' billions',\n",
              "   ' thousands',\n",
              "   ' hundreds',\n",
              "   ' enormous'],\n",
              "  'top_tokens_logits': array([5.3857775, 5.372451 , 5.3352838, 5.2637167, 4.7408137],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' millions',\n",
              "   ' thousands',\n",
              "   ' hundreds',\n",
              "   ' billions',\n",
              "   ' enormous'],\n",
              "  'top_tokens_logits': array([5.5317783, 5.5004086, 5.44882  , 5.388328 , 4.8500633],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' millions',\n",
              "   ' thousands',\n",
              "   ' hundreds',\n",
              "   ' billions',\n",
              "   ' enormous'],\n",
              "  'top_tokens_logits': array([5.360399 , 5.3432474, 5.23012  , 5.1817517, 5.016206 ],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' 100',\n",
              "   ' millions',\n",
              "   ' information',\n",
              "   ' thousands',\n",
              "   ' enormous'],\n",
              "  'top_tokens_logits': array([5.2604027, 5.249785 , 5.1972218, 5.172793 , 5.1623125],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' 100', ' tr', ' over', ' up', ' 1'],\n",
              "  'top_tokens_logits': array([7.136442 , 6.881729 , 6.859533 , 6.7607327, 6.469674 ],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' information', ' up', ' over', ' a', ' more'],\n",
              "  'top_tokens_logits': array([21.022247, 19.918947, 19.703325, 19.493725, 19.417505],\n",
              "        dtype=float32)}]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = logit_lens(model, \"In the context of this text, 'penguin' means 'electric car.' Penguins are now capable of traveling up to\", top_k=5)\n",
        "result[:]"
      ],
      "metadata": {
        "id": "zAkhICB4UdTw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = logit_lens(model, \"Here, 'mirror' is meant to signify 'blockchain technology.' Mirrors have become foundational in securing\", top_k=5)\n",
        "result[:]"
      ],
      "metadata": {
        "id": "lfuomaqyUe8p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = logit_lens(model, \"In this narrative, 'guitar' refers to 'virtual reality.' Guitars have been used to create immersive experiences that\", top_k=5)\n",
        "result[:]"
      ],
      "metadata": {
        "id": "fNHx7z2JUgQy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = logit_lens(model, \"In this discussion, 'rain' is a metaphor for 'data encryption.' Rain is crucial for protecting\", top_k=5)\n",
        "result[:]"
      ],
      "metadata": {
        "id": "SrlLgckVUITT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Observation\n",
        "\n",
        "Indeed, more meaningful tokens are present in last layers, the ones from first layers are meaningless. However, I noticed that sometimes the LM tries to take both meanings in, what results in generating more neutral response.  "
      ],
      "metadata": {
        "id": "psI9ptbKU0Xx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 2. Rotary embeddings\n",
        "\n",
        "**Task 2.1 (3 points)**\n",
        "\n",
        "In this task you'll need to code rotary embeddings. Actually, they are not just embeddings, but rather a transformation that is applied to queries and keys. It works like that:\n",
        "\n",
        "$$f_q(x_m, m) = x_mW_QR^d_{\\Theta, m},\\quad f_k(x_n, n) = x_nW_KR^d_{\\Theta, n},$$\n",
        "where\n",
        "$$R^d_{\\Theta, m} =\n",
        "\\begin{pmatrix}\n",
        "\\cos{m\\theta_1} & \\sin{m\\theta_1} & 0 & 0 & \\dots & 0 & 0\\\\\n",
        "-\\sin{m\\theta_1} & \\cos{m\\theta_1} & 0 & 0 & \\dots & 0 & 0\\\\\n",
        "0 & 0 & \\cos{m\\theta_2} & \\sin{m\\theta_2} & \\dots & 0 & 0\\\\\n",
        "0 & 0 & -\\sin{m\\theta_2} & \\cos{m\\theta_2} & \\dots & 0 & 0\\\\\n",
        "\\vdots & \\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\vdots\\\\\n",
        "0 & 0 & 0 & 0 & \\dots & \\cos{m\\theta_{d/2}} & \\sin{m\\theta_{d/2}}\\\\\n",
        "0 & 0 & 0 & 0 & \\dots & -\\sin{m\\theta_{d/2}} & \\cos{m\\theta_{d/2}}\\\\\n",
        "\\end{pmatrix},$$\n",
        "where the parameters $\\Theta$ are set to\n",
        "$$\\theta_i = b^{-2(i-1)/d}$$\n",
        "for some base $b$ (default $10000$).\n",
        "\n",
        "As you see, the transformation is the same for both keys and values, so we just need one transformation that takes a tensor of size `[batch_size, num_heads, seq_len, head_size]` and \"rotates\" it outputting a tensor of the same size."
      ],
      "metadata": {
        "id": "tPKjwA9KT60j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Please try your best to make calculations efficient and don't forget to use `torch` (and not `numpy`) and load all the tensors to the right `device`.\n",
        "\n",
        "Hints:\n",
        "1. As we've discussed in the longread, rotary embeddings can be $f_{q,k}(x_m, m) = x_mW_{Q,K}R^d_{\\Theta, m}$ of $f_{q,k}(x_m, m) = \\left(R^d_{\\Theta, m}\\right)^TW_{Q,K}x_m$ depending on whether $x_m$ is a row vector (first formula) or a column vector (second formula). In our case the input dimension is `[batch_size, num_heads, seq_len, head_size]`, so after we choose a particular batch element and a particular attention head, $x_m$ has dimension `(1, head_size)` which is a row.\n",
        "2. Recalculating all the $\\theta_i = b^{-2(i-1)/d}$, sines and cosines takes much time. A good ideas it to cache them as soon as they are needed. You can either calculate $R^d_{\\Theta, m}$ for large sequence length right at the initialization or:\n",
        "  - At initialization cache $R^d_{\\Theta, m}$ for moderately short sequences;\n",
        "  - When you encounter a longer sequence, recalculate and cache again.\n",
        "3. We don't need to store all the matrix $R^d_{\\Theta, m}$ because it has too many zeros. Actually, we can just store sines and cosines and do $x \\mapsto xR^d_{\\Theta, m}$ in linear time:\n",
        "$$xR^d_{\\Theta, m} = \\begin{pmatrix}\n",
        "x_1\\cos{m\\theta_1} - x_2\\sin{m\\theta_1}\\\\\n",
        "x_1\\sin{m\\theta_1} + x_2\\cos{m\\theta_1}\\\\\n",
        "x_3\\cos{m\\theta_2} - x_4\\sin{m\\theta_2}\\\\\n",
        "x_3\\sin{m\\theta_2} + x_4\\cos{m\\theta_2}\\\\\n",
        "\\vdots\n",
        "\\end{pmatrix} =\n",
        "\\begin{pmatrix}x_1\\\\x_2\\\\x_3\\\\x_4\\\\\\vdots\\end{pmatrix}\\otimes\n",
        "\\begin{pmatrix}\\cos{m\\theta_1}\\\\\\cos{m\\theta_1}\\\\\\cos{m\\theta_2}\\\\\\cos{m\\theta_2}\\\\\\vdots\\end{pmatrix} +\n",
        "\\begin{pmatrix}-x_2\\\\x_1\\\\-x_4\\\\x_2\\\\\\vdots\\end{pmatrix}\\otimes\n",
        "\\begin{pmatrix}\\sin{m\\theta_1}\\\\\\sin{m\\theta_1}\\\\\\sin{m\\theta_2}\\\\\\sin{m\\theta_2}\\\\\\vdots\\end{pmatrix}\n",
        "$$\n"
      ],
      "metadata": {
        "id": "RmHUFdyDY8Hp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RotaryEmbedding(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim: int,\n",
        "        max_position_embeddings: int,\n",
        "        base: int = 10_000,\n",
        "        device: Optional[torch.device] = None,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        pass\n",
        "\n",
        "    def forward(self, x: torch.Tensor, seq_len: Optional[int] = None):\n",
        "        # x: [batch_size, num_heads, seq_len, head_size]\n",
        "        # returns: [batch_size, num_heads, seq_len, head_size]\n",
        "        pass"
      ],
      "metadata": {
        "id": "_VyBg2gQWZAP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 2.2 (1 point)** Take a model from Hugging Face that has rotary embeddings. You can use `stabilityai/stablelm-zephyr-3b` or `mistralai/Mistral-7B-v0.1`, but many others will also work. Somewhere in `model.model.layers` you'll find the `RotaryEmbedding()` layer. Play around changing the `base` parameter. What do you observe if you make the base very small? Very large? How would the outputs of the model change? What would you expect to observe? Please don't only output sentences, but also provide some reflection."
      ],
      "metadata": {
        "id": "6ZsMSyyqgrqT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code here"
      ],
      "metadata": {
        "id": "Ux6t2pM-iRNi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 3. Going deeper into Mixtral\n",
        "\n",
        "In this task you'll try to understand better what happens inside the Mixtral of Experts model and, in the same time, fine tune it with QLoRA.\n",
        "\n",
        "**Caution**. Mixtral is quite large. It will consume >90 GB of disk space and ~23 GB VRAM on GPU when we load its 4-bit version. So, for a comfortable experience with this task, you will need A100 with 200GB disk space. If you don't have a way of finding this hardware, you can still do whatever doesn't require actually running the models, and there is a bonus task about Mistral for you in the end which you can do to get points.\n",
        "\n",
        "Apart from studying Mixtral, we will fine tune it. I've taken most of the fine tuning part from [this notebook](https://colab.research.google.com/github/brevdev/notebooks/blob/main/mixtral-finetune.ipynb#scrollTo=ece42f7c-3825-45c7-9afc-efb355e9474c).\n"
      ],
      "metadata": {
        "id": "p9Cbs6wH37nX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, let's load the libraries:"
      ],
      "metadata": {
        "id": "GwuhQpFo5TPo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U bitsandbytes\n",
        "!pip install -q -U git+https://github.com/huggingface/transformers.git\n",
        "!pip install -q -U git+https://github.com/huggingface/peft.git\n",
        "!pip install -q -U git+https://github.com/huggingface/accelerate.git\n",
        "!pip install -q -U datasets scipy ipywidgets matplotlib"
      ],
      "metadata": {
        "id": "42-7CMW75Z-n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from accelerate import FullyShardedDataParallelPlugin, Accelerator\n",
        "from torch.distributed.fsdp.fully_sharded_data_parallel import FullOptimStateDictConfig, FullStateDictConfig\n",
        "\n",
        "fsdp_plugin = FullyShardedDataParallelPlugin(\n",
        "    state_dict_config=FullStateDictConfig(offload_to_cpu=True, rank0_only=False),\n",
        "    optim_state_dict_config=FullOptimStateDictConfig(offload_to_cpu=True, rank0_only=False),\n",
        ")\n",
        "\n",
        "accelerator = Accelerator(fsdp_plugin=fsdp_plugin)"
      ],
      "metadata": {
        "id": "iAIwVqkq5hzZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For fine tuning we'll be using the [Viggo functional representation](https://huggingface.co/datasets/GEM/viggo) dataset. This dataset contains messages about video games such as\n",
        "\n",
        "*You said you loved The Legend of Zelda: Ocarina of Time. Do you often tend to play similar Nintendo games that are also rated E?*\n",
        "\n",
        "and meaning representations of such messages:\n",
        "\n",
        "*verify_attribute(name[The Legend of Zelda: Ocarina of Time], esrb[E (for Everyone)], rating[excellent], platforms[Nintendo])*\n",
        "\n",
        "We'll try to teach Mixtral to transform messages into their meaning representations. This is a good task for fine tuning, because it is about format tuning, not factuality."
      ],
      "metadata": {
        "id": "6YQcbzZx55ag"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "train_dataset = load_dataset('gem/viggo', split='train')\n",
        "eval_dataset = load_dataset('gem/viggo', split='validation')\n",
        "test_dataset = load_dataset('gem/viggo', split='test')"
      ],
      "metadata": {
        "id": "UfHZyaq75o1A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load base model\n",
        "\n",
        "We'll be downloading the model in full precision (so it will take ~100 GB on the disk) and then loading it in 4-bit quantization on GPU."
      ],
      "metadata": {
        "id": "sqPRhaJK-RLr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, DataCollatorForLanguageModeling, BitsAndBytesConfig\n",
        "\n",
        "base_model_id = \"mistralai/Mixtral-8x7B-v0.1\"\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(base_model_id, quantization_config=bnb_config, device_map=\"cuda\")"
      ],
      "metadata": {
        "id": "3ZzCtLm4-d1r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 3.1. [1 point]** Check which layers are 4-bit quantized and which are not. Actually, you can learn it by just typing `model` and printing its structure, but we encourage you to actually check the `dtype` to be sure about the result.\n",
        "\n",
        "You will find out that not all layers are quantized. Estimate the proportion of parameters that stay in full precision. Why these parameters aren't quantized? Any reasonable hypotesis will get points."
      ],
      "metadata": {
        "id": "RS-Mrtig_E0L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Your answer here]"
      ],
      "metadata": {
        "id": "Wk3QJbHrA3iY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inspecting the model\n",
        "\n",
        "**Task 3.2. [1 point]** Mixtral paper provides the following model chatacteristics:\n",
        "\n",
        "| Parameter | Value |\n",
        "| :--- |  ---: |\n",
        "| dim | 4096 |\n",
        "| n_layers | 32 |\n",
        "| head_dim | 128 |\n",
        "| hidden_dim | 14336 |\n",
        "| n_heads | 32 |\n",
        "| n_kv_heads | 128 |\n",
        "| context_len | 32768 |\n",
        "| vocab_size | 32000 |\n",
        "| num_experts | 8 |\n",
        "| top_k_experts | 2 |\n",
        "\n",
        "Some numbers can also be obtained by printing the `model`. Print it and browse through the dimensions.\n",
        "\n",
        "Some of the numbers are easies to understand. For example, we have 8 experts and 2 of them are used at inference. That's ok.\n",
        "\n",
        "Now, we have several questions for you:\n",
        "\n",
        "1. Why are the output dimensions (`out_features` in the model rollout) of `q_proj`, `k_proj` and `v_proj` different from 128 which is the dimension of a head (see table above)?\n",
        "2. Why are the output dimensions (`out_features` in the model rollout) of `k_proj` and `v_proj` different from the output dimensions of `q_proj`?\n",
        "\n",
        "Check the long read, it should be enough to answer all the questions."
      ],
      "metadata": {
        "id": "NuNjrpezcA06"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Your answer here]"
      ],
      "metadata": {
        "id": "00CtT9BZdbPf"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0doz6dkoRxcA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generating prompts for fine tuning"
      ],
      "metadata": {
        "id": "PqXekydTAxAA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    base_model_id,\n",
        "    add_eos_token=True,\n",
        "    add_bos_token=True,\n",
        ")\n",
        "\n",
        "def tokenize(prompt):\n",
        "    result = tokenizer(prompt)\n",
        "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
        "    return result"
      ],
      "metadata": {
        "id": "YmpiquPFBGNW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will be training Mixtral to transform text to meaning representation when prompted by a specific command. Namely, we'll fine tune it on prompts of the following form:\n",
        "\n",
        "```\n",
        "<s> Given a target sentence construct the underlying meaning representation of the input sentence as a single function with attributes and attribute values.\n",
        "This function should describe the target string accurately and the function must be one of the following ['inform', 'request', 'give_opinion', 'confirm', 'verify_attribute', 'suggest', 'request_explanation', 'recommend', 'request_attribute'].\n",
        "The attributes must be one of the following: ['name', 'exp_release_date', 'release_year', 'developer', 'esrb', 'rating', 'genres', 'player_perspective', 'has_multiplayer', 'platforms', 'available_on_steam', 'has_linux_release', 'has_mac_release', 'specifier']\n",
        "\n",
        "### Target sentence:\n",
        "Dirt: Showdown is a sport racing game that was released in 2012. The game is available on PlayStation, Xbox, and PC, and it has an ESRB Rating of E 10+ (for Everyone 10 and Older). However, it is not yet available as a Steam, Linux, or Mac release.\n",
        "\n",
        "### Meaning representation:\n",
        "inform(name[Dirt: Showdown], release_year[2012], esrb[E 10+ (for Everyone 10 and Older)], genres[driving/racing, sport], platforms[PlayStation, Xbox, PC], available_on_steam[no], has_linux_release[no], has_mac_release[no])\n",
        "</s>\n",
        "```"
      ],
      "metadata": {
        "id": "LrFNK6UmBGs2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For fine tuning we will be padding the texts, and for that we need to understand the distribution of lengths"
      ],
      "metadata": {
        "id": "efknjmVMDrhs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_data_lengths(tokenized_train_dataset, tokenized_val_dataset):\n",
        "    lengths = [len(x['input_ids']) for x in tokenized_train_dataset]\n",
        "    lengths += [len(x['input_ids']) for x in tokenized_val_dataset]\n",
        "    print(len(lengths))\n",
        "\n",
        "    # Plotting the histogram\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    plt.hist(lengths, bins=20, alpha=0.7, color='blue')\n",
        "    plt.xlabel('Length of input_ids')\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.title('Distribution of Lengths of input_ids')\n",
        "    plt.show()\n",
        "\n",
        "plot_data_lengths(tokenized_train_dataset, tokenized_val_dataset)"
      ],
      "metadata": {
        "id": "hwH5EQrFCdvn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You will find that 340 is a good estimate of max length. We will include padding and truncation into the `tokenization` routine:"
      ],
      "metadata": {
        "id": "3TL3ONExDynK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_length = 340 # This was an appropriate max length for my dataset\n",
        "\n",
        "# redefine the tokenize function and tokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    base_model_id,\n",
        "    padding_side=\"left\",\n",
        "    add_eos_token=True,\n",
        "    add_bos_token=True,\n",
        ")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "\n",
        "def tokenize(prompt):\n",
        "    result = tokenizer(\n",
        "        prompt,\n",
        "        truncation=True,\n",
        "        max_length=max_length,\n",
        "        padding=\"max_length\",\n",
        "    )\n",
        "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
        "    return result"
      ],
      "metadata": {
        "id": "azadmjD9ENmu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And now, let's assemble the prompts for fine tuning:"
      ],
      "metadata": {
        "id": "Io4Dxb3eEORR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_and_tokenize_prompt(data_point):\n",
        "    full_prompt =f\"\"\"Given a target sentence construct the underlying meaning representation of the input sentence as a single function with attributes and attribute values.\n",
        "This function should describe the target string accurately and the function must be one of the following ['inform', 'request', 'give_opinion', 'confirm', 'verify_attribute', 'suggest', 'request_explanation', 'recommend', 'request_attribute'].\n",
        "The attributes must be one of the following: ['name', 'exp_release_date', 'release_year', 'developer', 'esrb', 'rating', 'genres', 'player_perspective', 'has_multiplayer', 'platforms', 'available_on_steam', 'has_linux_release', 'has_mac_release', 'specifier']\n",
        "\n",
        "### Target sentence:\n",
        "{data_point[\"target\"]}\n",
        "\n",
        "### Meaning representation:\n",
        "{data_point[\"meaning_representation\"]}\n",
        "\"\"\"\n",
        "    return tokenize(full_prompt)"
      ],
      "metadata": {
        "id": "5hSWcYBoBIjZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_train_dataset = train_dataset.map(generate_and_tokenize_prompt)\n",
        "tokenized_val_dataset = eval_dataset.map(generate_and_tokenize_prompt)"
      ],
      "metadata": {
        "id": "o5MNiiD7Bhb5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Running the model for the first time\n",
        "\n",
        "Now, let's try to apply Mixtral!"
      ],
      "metadata": {
        "id": "iyVrvtccFXCY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eval_prompt = \"\"\"Given a target sentence construct the underlying meaning representation of the input sentence as a single function with attributes and attribute values.\n",
        "This function should describe the target string accurately and the function must be one of the following ['inform', 'request', 'give_opinion', 'confirm', 'verify_attribute', 'suggest', 'request_explanation', 'recommend', 'request_attribute'].\n",
        "The attributes must be one of the following: ['name', 'exp_release_date', 'release_year', 'developer', 'esrb', 'rating', 'genres', 'player_perspective', 'has_multiplayer', 'platforms', 'available_on_steam', 'has_linux_release', 'has_mac_release', 'specifier']\n",
        "\n",
        "### Target sentence:\n",
        "Earlier, you stated that you didn't have strong feelings about PlayStation's Little Big Adventure. Is your opinion true for all games which don't have multiplayer?\n",
        "\n",
        "### Meaning representation:\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "E9P4fP5XFxXR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can apply accelerator to the model. If you're using colab, it can start raising stupid errors like\n",
        "\n",
        "```\n",
        "TypeError: device() received an invalid combination of arguments - got (NoneType), but expected one of:\n",
        " * (torch.device device)\n",
        "      didn't match because some of the arguments have invalid types: (!NoneType!)\n",
        " * (str type, int index)\n",
        "\n",
        "```\n",
        "\n",
        "Most likely, you'll be able to overcome it by running\n",
        "\n",
        "`model.hf_device_map = {'': torch.device('cuda', index=0)}`"
      ],
      "metadata": {
        "id": "V5BeR4y6F03x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply the accelerator. You can comment this out to remove the accelerator.\n",
        "model = accelerator.prepare_model(model)"
      ],
      "metadata": {
        "id": "SNzB8UTFFx8v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Re-init the tokenizer so it doesn't add padding or eos token\n",
        "eval_tokenizer = AutoTokenizer.from_pretrained(\n",
        "    base_model_id,\n",
        "    add_bos_token=True,\n",
        ")"
      ],
      "metadata": {
        "id": "hEtbvs6ZIZ3P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\"\n",
        "model_input = eval_tokenizer(eval_prompt, return_tensors=\"pt\").to(device)"
      ],
      "metadata": {
        "id": "2BPeKOzwIjJo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    print(eval_tokenizer.decode(model.generate(**model_input, max_new_tokens=128)[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "id": "PMagIWF9Iji4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can see that the model doesn't understand which formatting we are expecting from it. We will try to improve it with fine tuning."
      ],
      "metadata": {
        "id": "xm48VG57P2Z_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Investigating Mixtral heads\n",
        "\n",
        "**Task 3.3. [2 points]** Apply Mixtral to some sentence\n",
        "\n",
        "`router_logits = model(**model_input, output_router_logits=True)`\n",
        "\n",
        "to make it return router logits that decide which heads are used for inference.\n",
        "\n",
        "Do the logits stay the same when you apply Mixtral several times to the same sentence? Why?\n",
        "\n",
        "Now:\n",
        "- get router logits of 10th, 20th and 30th layers for the first 100 elements of `tokenized_train_dataset`.\n",
        "- For each expert of each of these layers, make a list of tokens for which this expert has top-1 logit (yes, please do it separately for each of the experts),\n",
        "- Output top-5 most frequent tokens from each of the lists.\n",
        "\n",
        "Your output should be like:\n",
        "\n",
        "```\n",
        "At layer 20:\n",
        "\n",
        "Expert 0:\n",
        "[('token1', how_many_times_expert_0_got_top1_router_logit_with_this_token),\n",
        "('token2', how_many_times_expert_0_got_top1_router_logit_with_this_token),\n",
        "('token3', how_many_times_expert_0_got_top1_router_logit_with_this_token),\n",
        "('token4', how_many_times_expert_0_got_top1_router_logit_with_this_token),\n",
        "('token5', how_many_times_expert_0_got_top1_router_logit_with_this_token),]\n",
        "\n",
        "Expert 1\n",
        "....\n",
        "```\n",
        "\n",
        "Do you observe any patterns?"
      ],
      "metadata": {
        "id": "TVU3GGRSIllV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# <YOUR CODE HERE>"
      ],
      "metadata": {
        "id": "TnwxQnAKRzzO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[YOUR ANSWER HERE]"
      ],
      "metadata": {
        "id": "adNCHnjXR2Rk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set Up LoRA\n",
        "\n",
        "We will be fine tuning our model with LoRA. To start, we have to apply some preprocessing to the model to prepare it for training. For that use the prepare_model_for_kbit_training method from PEFT."
      ],
      "metadata": {
        "id": "rrIlFIiY9EoI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import prepare_model_for_kbit_training\n",
        "\n",
        "model.gradient_checkpointing_enable()\n",
        "model = prepare_model_for_kbit_training(model)"
      ],
      "metadata": {
        "id": "KPe8MODnK8vS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we'll define which layers are subject to fine tuning and understand how many trainable parameters we are going to have."
      ],
      "metadata": {
        "id": "fQO5t9qY9R6A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def print_trainable_parameters(model):\n",
        "    \"\"\"\n",
        "    Prints the number of trainable parameters in the model.\n",
        "    \"\"\"\n",
        "    trainable_params = 0\n",
        "    all_param = 0\n",
        "    for _, param in model.named_parameters():\n",
        "        all_param += param.numel()\n",
        "        if param.requires_grad:\n",
        "            trainable_params += param.numel()\n",
        "    print(\n",
        "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
        "    )"
      ],
      "metadata": {
        "id": "r--3XXmz9M1B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    target_modules=[\n",
        "        \"q_proj\",\n",
        "        \"k_proj\",\n",
        "        \"v_proj\",\n",
        "        \"o_proj\",\n",
        "        \"w1\",\n",
        "        \"w2\",\n",
        "        \"w3\",\n",
        "        \"lm_head\",\n",
        "    ],\n",
        "    bias=\"none\",\n",
        "    lora_dropout=0.05,  # Conventional\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n"
      ],
      "metadata": {
        "id": "hPOEbNqu9kWJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = get_peft_model(model, config)\n",
        "print_trainable_parameters(model)"
      ],
      "metadata": {
        "id": "PpOnXfu8R9ob"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you see, only a small ratio of parameters are trainable."
      ],
      "metadata": {
        "id": "H2B14IhNR_O6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's look closer at the config.\n",
        "\n",
        "* `r` is the rank of the low-rank matrix used in the adapters. The larger it is, the more trainable parameters we have, so the more expressive the model is, but also the more compute we need for fine tuning. We set `r = 8` which is a reasonable default value.\n",
        "* `lora_alpha` is the scaling factor for the learned weights. The weight matrix $\\Delta W$ is scaled by `lora_alpha/r`, and thus a higher value for alpha assigns more weight to the LoRA activations. We use `lora_alpha = 16` which is also a reasonable default.\n",
        "\n",
        "\n",
        "The trainable layers are indicated by their codes \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"w1\", \"w2\", \"w3\", \"lm_head\". Here,\n",
        "\n",
        "* \"q_proj\", \"k_proj\", \"v_proj\" are $W_Q, W_K, W_V$ matrices making queries, keys and values from transformer layer inputs like in $q = xW_Q$.\n",
        "* \"o_proj\" is the output projection matrix $W_o$ that comes at the final step of QKV-attention mechanism. For the linearized attention mechanism it would look like this:\n",
        "$$o_n = \\sum_{m=1}^N\\frac{\\left(\\psi(q_n)R^d_{\\Theta, n}\\right)\\left(\\phi(k_m)R^d_{\\Theta, m}\\right)^T}{\\sum_{m=1}^N\\psi(q_n)\\phi(k_m)^T}v_m,$$\n",
        "$$\\quad$$\n",
        "$$output = oW_o,$$\n",
        "* \"lm_head\" is the language modeling head that goes after the last transformer block and predicts next token logits.\n",
        "\n",
        "It is reasonable that we don't fine tune the embedding layer. It is only reasonable to train it if you introduce new tokens (for example, if you want to adapt your model to new languages), and in this case we only train the embeddings of newly added tokens."
      ],
      "metadata": {
        "id": "dPzHiX6b9k2b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 3.4. [1 point]** Now, we have a quest for you. The labels \"w1\", \"w2\" and \"w3\" stand for the linear layers inside each of the experts (who are just MLPs). But how are they connected to each other? What is the architecture of this MLP?\n",
        "\n",
        "It's not addressed well in the Mixtral/Mistral papers (feel free to check though, maybe the author of this hometask wasn't attentive enough), so your best chance to grasp it is finding the source code in the Transformers library."
      ],
      "metadata": {
        "id": "7C3ktN8v_sQ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[YOUR ANSWER HERE]"
      ],
      "metadata": {
        "id": "MEx5KQCAR67a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run Training\n",
        "\n",
        "Fine tuning can take some time. However, you can stop training earlier (say, after 100-500 steps) if you observe overfitting or are just tired of waiting. Thought, the result can be not so great."
      ],
      "metadata": {
        "id": "N6YPuXtWJ-aQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.device_count() > 1: # If more than 1 GPU\n",
        "    model.is_parallelizable = True\n",
        "    model.model_parallel = True"
      ],
      "metadata": {
        "id": "e1ZtEQq2Jyap"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.device_count() # should be 4 if using Brev's instance link"
      ],
      "metadata": {
        "id": "TlFcqYNZMxRc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "from datetime import datetime\n",
        "\n",
        "project = \"viggo-finetune\"\n",
        "base_model_name = \"mixtral\"\n",
        "run_name = base_model_name + \"-\" + project\n",
        "output_dir = \"./\" + run_name\n",
        "\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "trainer = transformers.Trainer(\n",
        "    model=model,\n",
        "    train_dataset=tokenized_train_dataset,\n",
        "    eval_dataset=tokenized_val_dataset,\n",
        "    args=transformers.TrainingArguments(\n",
        "        output_dir=output_dir,\n",
        "        warmup_steps=5,\n",
        "        per_device_train_batch_size=1,\n",
        "        gradient_checkpointing=True,\n",
        "        gradient_accumulation_steps=4,\n",
        "        max_steps=1000,\n",
        "        learning_rate=2.5e-5,\n",
        "        logging_steps=25,\n",
        "        fp16=True,\n",
        "        optim=\"paged_adamw_8bit\",\n",
        "        logging_dir=\"./logs\",        # Directory for storing logs\n",
        "        save_strategy=\"steps\",       # Save the model checkpoint every logging step\n",
        "        save_steps=50,                # Save checkpoints every 50 steps\n",
        "        evaluation_strategy=\"steps\", # Evaluate the model every logging step\n",
        "        eval_steps=50,               # Evaluate and save checkpoints every 50 steps\n",
        "        do_eval=True,                # Perform evaluation at the end of training\n",
        "        # report_to=\"wandb\",           # Comment this out if you don't want to use weights & baises\n",
        "        run_name=f\"{run_name}-{datetime.now().strftime('%Y-%m-%d-%H-%M')}\"          # Name of the W&B run (optional)\n",
        "    ),\n",
        "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
        ")\n",
        "\n",
        "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!"
      ],
      "metadata": {
        "id": "jL4N__SGMxPC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 3.5. [1 point]** Run the training!"
      ],
      "metadata": {
        "id": "x8TfshujM780"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "DSkYaUNUM52B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that while training the model will save the adapter weights each 50 steps (`save_steps` param) in the folders like \"mixtral-viggo-finetune/checkpoint-50\", \"mixtral-viggo-finetune-2/checkpoint-100\" etc. This way, you'll be able to restore the trained model later."
      ],
      "metadata": {
        "id": "ysB7pTsSN5mS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Try the model\n",
        "\n",
        "At this point, we recommend you to restart the kernel to avoid running into out of memory problems.\n",
        "\n",
        "To reload the model, we need to do two things:\n",
        "\n",
        "1. Load the base model,\n",
        "2. Load the adapter weights that were saved in the checkpoints."
      ],
      "metadata": {
        "id": "o-NwafE9Nj0O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "\n",
        "base_model_id = \"mistralai/Mistral-7B-v0.1\"\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model_id,  # Mixtral, same as before\n",
        "    quantization_config=bnb_config,  # Same quantization config as before\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "\n",
        "eval_tokenizer = AutoTokenizer.from_pretrained(\n",
        "    base_model_id,\n",
        "    add_bos_token=True,\n",
        "    trust_remote_code=True,\n",
        ")"
      ],
      "metadata": {
        "id": "rCrCl_L1NlJZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import PeftModel\n",
        "\n",
        "# Insert correct checkpoint index, double check the path\n",
        "ft_model = PeftModel.from_pretrained(base_model, \"mixtral-viggo-finetune/checkpoint-50\")"
      ],
      "metadata": {
        "id": "Csyz7wZYO9fg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's run the model and see if it learnt to abide the required format:"
      ],
      "metadata": {
        "id": "K6cTbanrPZMd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eval_prompt = \"\"\"Given a target sentence construct the underlying meaning representation of the input sentence as a single function with attributes and attribute values.\n",
        "This function should describe the target string accurately and the function must be one of the following ['inform', 'request', 'give_opinion', 'confirm', 'verify_attribute', 'suggest', 'request_explanation', 'recommend', 'request_attribute'].\n",
        "The attributes must be one of the following: ['name', 'exp_release_date', 'release_year', 'developer', 'esrb', 'rating', 'genres', 'player_perspective', 'has_multiplayer', 'platforms', 'available_on_steam', 'has_linux_release', 'has_mac_release', 'specifier']\n",
        "\n",
        "### Target sentence:\n",
        "Earlier, you stated that you didn't have strong feelings about PlayStation's Little Big Adventure. Is your opinion true for all games which don't have multiplayer?\n",
        "\n",
        "### Meaning representation:\n",
        "\"\"\"\n",
        "\n",
        "model_input = eval_tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "ft_model.eval()\n",
        "with torch.no_grad():\n",
        "    print(eval_tokenizer.decode(ft_model.generate(**model_input, max_new_tokens=50)[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "id": "CRFY-lXkPoLv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 4*. Do this instead of Task 3 if Mixtral doesn't fit on your GPU\n",
        "\n",
        "If you can't approach Mixtral, you will do almost the same with Mistral. Mistral is much less compute intensive and fits well on V100.\n",
        "\n",
        "In what concerns fine tuning, the only thing you need is to change `model_id` to `\"mistralai/Mistral-7B-v0.1\"`. All the code should work without trouble. Just **please indicate in bold in the beginning of Task 3 that you're using Mistral instead of Mixtral**.\n",
        "\n",
        "**Tasks 3.1, 3.2, 3.4, and 3.5** stay the same. Moreover, the answers won't change much, because Mixtral inherits Mistral's architectural ideas.\n",
        "\n",
        "Task 3.3 doean't make sense with Mistral, so instead you'll do the following:\n",
        "\n",
        "**Task 4.3. [2 points]** Fine tuning Mistral is faster than fine tuning Mixtral, so you can run several experiments (but try to reach at least 500 steps). Train only attention layers with QLoRA. How will the number of trainable parameters change? Compare the quality of the resulting model."
      ],
      "metadata": {
        "id": "QZkuhFYqQL7c"
      }
    }
  ]
}